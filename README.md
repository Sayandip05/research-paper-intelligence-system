# ðŸ”¬ Research Paper Intelligence System

A production-ready **Multimodal Hybrid RAG (Retrieval-Augmented Generation)** system for intelligent research paper analysis using **LlamaIndex**, **Qdrant**, **CLIP**, and **Multi-Agent Workflows** with comprehensive observability via **Langfuse**.

## ðŸŽ¯ Project Overview

This system ingests PDF research papers, extracts both text and images, generates multimodal embeddings (dense + sparse + CLIP), and enables intelligent Q&A with proper citations. It features a **3-agent workflow** with intent classification, section-filtered retrieval, human-in-the-loop controls, and full observability.

```mermaid
flowchart TB
    subgraph Ingestion["ðŸ“¥ Ingestion Pipeline"]
        PDF[PDF Files] --> Parser[Section-Aware Parser]
        Parser --> Chunker[Smart Chunker]
        Chunker --> TextEmb[Dense: BGE-768]
        Chunker --> SparseEmb[Sparse: BM42]
        PDF --> ImageExt[Image Extractor]
        ImageExt --> CLIPEmb[CLIP: ViT-B/32-512]
    end
    
    subgraph Storage["ðŸ’¾ Vector Storage"]
        TextEmb --> Qdrant[(Qdrant Hybrid)]
        SparseEmb --> Qdrant
        CLIPEmb --> QdrantImg[(Qdrant Images)]
    end
    
    subgraph Query["ðŸ” Query Pipeline"]
        User[User Question] --> Orchestrator[Query Orchestrator]
        Orchestrator --> Retriever[Evidence Retrieval]
        Retriever --> Qdrant
        Retriever --> QdrantImg
        Retriever --> HITL{HITL Gate}
        HITL --> Analyzer[Analysis & Synthesis]
        Analyzer --> Guardrails[Guardrails AI]
        Guardrails --> Response[Validated Response]
    end
    
    subgraph Observability["ðŸ“Š Observability"]
        Orchestrator --> Langfuse[(Langfuse)]
        Retriever --> Langfuse
        Analyzer --> Langfuse
        Guardrails --> Langfuse
    end
    
    style Ingestion fill:#e1f5ff
    style Storage fill:#fff4e1
    style Query fill:#e8f5e9
    style Observability fill:#f3e5f5
```

---

## ðŸ—ï¸ Detailed System Architecture

### High-Level Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PRESENTATION LAYER                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Streamlit UI    â”‚  â”‚ Interactive CLI â”‚  â”‚ FastAPI REST API            â”‚  â”‚
â”‚  â”‚ (ChatGPT-style) â”‚  â”‚ (Terminal)      â”‚  â”‚ (Swagger/OpenAPI)           â”‚  â”‚
â”‚  â”‚ Port: 8501      â”‚  â”‚ Python script   â”‚  â”‚ Port: 8000                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                   â”‚                         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENT WORKFLOW LAYER                      â”‚
â”‚              (LlamaIndex Workflow with @step)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚  STEP 1: Query  â”‚â”€â”€â”€â”€â–¶â”‚  STEP 2:        â”‚              â”‚
â”‚   â”‚  Orchestrator   â”‚     â”‚  Evidence       â”‚              â”‚
â”‚   â”‚                 â”‚     â”‚  Retrieval      â”‚              â”‚
â”‚   â”‚ â€¢ Intent Class  â”‚     â”‚                 â”‚              â”‚
â”‚   â”‚ â€¢ Section Map   â”‚     â”‚ â€¢ Hybrid Search â”‚              â”‚
â”‚   â”‚ â€¢ Confidence    â”‚     â”‚ â€¢ Image Search  â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                    â”‚                        â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                           â”‚    HITL Gate    â”‚              â”‚
â”‚                           â”‚  (Deterministic)â”‚              â”‚
â”‚                           â”‚                 â”‚              â”‚
â”‚                           â”‚ â€¢ Min chunks: 2 â”‚              â”‚
â”‚                           â”‚ â€¢ Confidence    â”‚              â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                    â”‚                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚  STEP 4: Human  â”‚â—€â”€â”€â”€â”€â”‚  STEP 3:        â”‚              â”‚
â”‚   â”‚  Review Handler â”‚     â”‚  Analysis &     â”‚              â”‚
â”‚   â”‚                 â”‚     â”‚  Synthesis      â”‚              â”‚
â”‚   â”‚ â€¢ Auto-approve  â”‚     â”‚                 â”‚              â”‚
â”‚   â”‚ â€¢ Escalation    â”‚     â”‚ â€¢ LLM Reasoning â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â€¢ Citation      â”‚              â”‚
â”‚                           â”‚ â€¢ Confidence    â”‚              â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVICE LAYER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              EMBEDDING SERVICES                          â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Dense (BGE)      â”‚ Sparse (BM42)    â”‚ CLIP (Images)    â”‚â”‚
â”‚  â”‚ â€¢ BAAI/bge-base  â”‚ â€¢ fastembed      â”‚ â€¢ ViT-B/32       â”‚â”‚
â”‚  â”‚ â€¢ 768-dim        â”‚ â€¢ Keyword match  â”‚ â€¢ 512-dim        â”‚â”‚
â”‚  â”‚ â€¢ Semantic       â”‚ â€¢ BM42 algorithm â”‚ â€¢ Cross-modal    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              PROCESSING SERVICES                         â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ PDF Parser   â”‚ Chunking     â”‚ Intent       â”‚ LLM       â”‚â”‚
â”‚  â”‚ â€¢ PyMuPDF    â”‚ â€¢ Sentence   â”‚ â€¢ Rule-based â”‚ â€¢ Groq    â”‚â”‚
â”‚  â”‚ â€¢ Section    â”‚   Splitter   â”‚ â€¢ 10 intents â”‚ â€¢ GPT-4   â”‚â”‚
â”‚  â”‚   detection  â”‚ â€¢ 1000 char  â”‚ â€¢ Priority   â”‚   style   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              QUALITY SERVICES                            â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Guardrails AI          â”‚ HITL Gate                      â”‚â”‚
â”‚  â”‚ â€¢ Pydantic validation  â”‚ â€¢ Rule-based checks            â”‚â”‚
â”‚  â”‚ â€¢ Citation grounding   â”‚ â€¢ Confidence thresholds        â”‚â”‚
â”‚  â”‚ â€¢ Hallucination detect â”‚ â€¢ Auto-escalation              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA LAYER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ QDRANT VECTOR DATABASE       â”‚  â”‚ MONGODB             â”‚  â”‚
â”‚  â”‚ â”œâ”€ research_papers_hybrid    â”‚  â”‚ â”œâ”€ sessions         â”‚  â”‚
â”‚  â”‚ â”‚  â”œâ”€ text-dense (768-dim)   â”‚  â”‚ â”œâ”€ messages         â”‚  â”‚
â”‚  â”‚ â”‚  â””â”€ sparse (BM42)          â”‚  â”‚ â””â”€ metadata         â”‚  â”‚
â”‚  â”‚ â””â”€ research_papers_images    â”‚  â”‚                     â”‚  â”‚
â”‚  â”‚    â””â”€ clip (512-dim)         â”‚  â”‚                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š Langfuse Observability Architecture

### Comprehensive Tracing Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LANGFUSE OBSERVABILITY                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    TRACING LAYERS                                    â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  LAYER 1: WORKFLOW TRACING                                          â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Research_Workflow_Execute")                       â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Workflow_Step1_Orchestrate")                      â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Workflow_Step2_Retrieve")                         â”‚   â”‚
â”‚  â”‚  â””â”€ @observe(name="Workflow_Step3_Analyze")                          â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  LAYER 2: AGENT TRACING                                             â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Agent_QueryOrchestrator")                         â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Agent_EvidenceRetrieval")                         â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Agent_AnalysisSynthesis")                         â”‚   â”‚
â”‚  â”‚  â””â”€ @observe(name="Intent_Classification")                           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  LAYER 3: SERVICE TRACING                                           â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="RAG_Query")                                       â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Qdrant_Retrieval")                                â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="CLIP_Image_Retrieval")                            â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Stateless_Query")                                 â”‚   â”‚
â”‚  â”‚  â””â”€ @observe(name="Session_Query")                                   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  LAYER 4: GUARDRAILS TRACING                                        â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Guardrails_ValidateEnforce")                      â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Guardrails_SchemaValidation")                     â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Guardrails_CitationGrounding")                    â”‚   â”‚
â”‚  â”‚  â””â”€ @observe(name="Guardrails_HallucinationCheck")                   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  LAYER 5: API TRACING                                               â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Hybrid_Search")                                   â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="Dense_Search")                                    â”‚   â”‚
â”‚  â”‚  â”œâ”€ @observe(name="PDF_Upload")                                      â”‚   â”‚
â”‚  â”‚  â””â”€ @observe(name="PDF_Processing")                                  â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    INTEGRATION POINTS                                â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  1. LLAMAINdex Callback Handler                                     â”‚   â”‚
â”‚  â”‚     File: backend/app/main.py                                        â”‚   â”‚
â”‚  â”‚     - LlamaIndexCallbackHandler integrated with Settings             â”‚   â”‚
â”‚  â”‚     - Captures all LlamaIndex operations                             â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  2. Decorator-Based Tracing                                         â”‚   â”‚
â”‚  â”‚     File: backend/app/services/langfuse_utils.py                     â”‚   â”‚
â”‚  â”‚     - @observe decorator from langfuse                               â”‚   â”‚
â”‚  â”‚     - Automatic span creation and context propagation                â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  3. Lifecycle Management                                            â”‚   â”‚
â”‚  â”‚     - Initialization: On FastAPI startup                             â”‚   â”‚
â”‚  â”‚     - Flushing: On FastAPI shutdown (graceful)                       â”‚   â”‚
â”‚  â”‚     - Ensures no trace data loss                                     â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    CONFIGURATION                                     â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  Environment Variables:                                             â”‚   â”‚
â”‚  â”‚  â”œâ”€ LANGFUSE_PUBLIC_KEY     # Your Langfuse public key               â”‚   â”‚
â”‚  â”‚  â”œâ”€ LANGFUSE_SECRET_KEY     # Your Langfuse secret key               â”‚   â”‚
â”‚  â”‚  â”œâ”€ LANGFUSE_HOST           # http://localhost:3000                  â”‚   â”‚
â”‚  â”‚  â””â”€ ENABLE_LANGFUSE         # true/false                             â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  File: backend/app/config.py                                         â”‚   â”‚
â”‚  â”‚  Settings class manages all Langfuse configuration                   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ Complete Project Structure

```
research-paper-intelligence-system/
â”‚
â”œâ”€â”€ ðŸ“‚ corpus/                                    # PDF storage folder
â”‚   â”œâ”€â”€ paper1.pdf
â”‚   â””â”€â”€ paper2.pdf
â”‚
â”œâ”€â”€ ðŸ“‚ backend/                                   # FastAPI backend
â”‚   â””â”€â”€ ðŸ“‚ app/
â”‚       â”‚
â”‚       â”œâ”€â”€ ðŸ“‚ agents/                            # Multi-Agent System
â”‚       â”‚   â”œâ”€â”€ query_orchestrator.py            # Agent 1: Intent & Planning
â”‚       â”‚   â”œâ”€â”€ evidence_retrieval.py            # Agent 2: Hybrid Retrieval
â”‚       â”‚   â””â”€â”€ analysis_synthesis.py            # Agent 3: LLM Reasoning
â”‚       â”‚
â”‚       â”œâ”€â”€ ðŸ“‚ api/
â”‚       â”‚   â””â”€â”€ ðŸ“‚ routes/                        # REST API Endpoints
â”‚       â”‚       â”œâ”€â”€ query.py                     # /api/query endpoints
â”‚       â”‚       â”œâ”€â”€ search.py                    # /api/search endpoints
â”‚       â”‚       â”œâ”€â”€ upload.py                    # /api/upload endpoint
â”‚       â”‚       â”œâ”€â”€ sessions.py                  # /api/sessions endpoints
â”‚       â”‚       â”œâ”€â”€ image_search.py             # /api/image-search
â”‚       â”‚       â””â”€â”€ images.py                    # /api/image serving
â”‚       â”‚
â”‚       â”œâ”€â”€ ðŸ“‚ db/                               # Database Clients
â”‚       â”‚   â”œâ”€â”€ qdrant_client.py                # Qdrant vector operations
â”‚       â”‚   â””â”€â”€ mongo_client.py                 # MongoDB connection
â”‚       â”‚
â”‚       â”œâ”€â”€ ðŸ“‚ models/                           # Pydantic Data Models
â”‚       â”‚   â”œâ”€â”€ events.py                       # Workflow event types
â”‚       â”‚   â”œâ”€â”€ paper.py                        # Paper & section models
â”‚       â”‚   â”œâ”€â”€ chunk.py                        # Text chunk models
â”‚       â”‚   â”œâ”€â”€ image.py                        # Image metadata models
â”‚       â”‚   â”œâ”€â”€ session.py                      # Chat session models
â”‚       â”‚   â””â”€â”€ query.py                        # Query request/response
â”‚       â”‚
â”‚       â”œâ”€â”€ ðŸ“‚ services/                         # Core Business Logic
â”‚       â”‚   â”œâ”€â”€ llm_service.py                  # Groq LLM integration
â”‚       â”‚   â”œâ”€â”€ embeddings.py                   # BGE + BM42 embeddings
â”‚       â”‚   â”œâ”€â”€ clip_embedding.py              # CLIP image embeddings
â”‚       â”‚   â”œâ”€â”€ pdf_parser.py                  # Section-aware PDF parsing
â”‚       â”‚   â”œâ”€â”€ chunking.py                    # Text chunking service
â”‚       â”‚   â”œâ”€â”€ image_extraction.py            # PDF image extraction
â”‚       â”‚   â”œâ”€â”€ intent_classifier.py           # Rule-based intent detection
â”‚       â”‚   â”œâ”€â”€ hitl_gate.py                   # Human-in-the-loop logic
â”‚       â”‚   â”œâ”€â”€ guardrails_service.py          # Output validation
â”‚       â”‚   â”œâ”€â”€ query_engine.py                # RAG query engine
â”‚       â”‚   â”œâ”€â”€ session_service.py             # Chat session management
â”‚       â”‚   â””â”€â”€ langfuse_utils.py              # Observability utilities
â”‚       â”‚
â”‚       â”œâ”€â”€ ðŸ“‚ workflows/                        # Workflow Orchestration
â”‚       â”‚   â””â”€â”€ research_workflow.py           # LlamaIndex workflow
â”‚       â”‚
â”‚       â”œâ”€â”€ config.py                           # Application configuration
â”‚       â””â”€â”€ main.py                             # FastAPI application entry
â”‚
â”œâ”€â”€ ðŸ“‚ frontend/                                 # Streamlit UI
â”‚   â””â”€â”€ app.py                                  # ChatGPT-style interface
â”‚
â”œâ”€â”€ ðŸ“„ build_corpus.py                          # CLI corpus builder
â”œâ”€â”€ ðŸ“„ interactive_query.py                     # CLI query interface
â”œâ”€â”€ ðŸ“„ docker-compose.yml                       # Infrastructure (Qdrant, MongoDB)
â”œâ”€â”€ ðŸ“„ requirements.txt                         # Python dependencies
â”œâ”€â”€ ðŸ“„ .env                                     # Environment variables
â””â”€â”€ ðŸ“„ README.md                                # This file
```

---

## ðŸ” Detailed File Documentation

### 1. Agent Layer (`backend/app/agents/`)

#### `query_orchestrator.py` (Agent 1: The Brain)
**Purpose**: Analyzes user questions and plans retrieval strategy without retrieving documents.

**Key Components**:
- `QueryOrchestratorAgent`: Main agent class
- `IntentType` enum: SUMMARY, COMPARISON, RESEARCH_GAPS
- `_classify_intent()`: LLM-based intent classification
- `_determine_sections()`: Maps intent to paper sections
- `_predict_human_review_needed()`: Predicts if HITL might be needed

**Flow**:
```
StartEvent â†’ Classify Intent â†’ Determine Sections â†’ Set Threshold â†’ RetrievalEvent
```

**Dependencies**:
- `app.models.events`: StartEvent, RetrievalEvent, IntentType
- `app.services.llm_service`: Groq LLM for classification
- `langfuse.decorators`: @observe for tracing

---

#### `evidence_retrieval.py` (Agent 2: The Retriever)
**Purpose**: Retrieves both text chunks and images using hybrid search.

**Key Components**:
- `EvidenceRetrievalAgent`: Multimodal retrieval agent
- `process()`: Main retrieval method (returns AnalysisEvent or HumanReviewEvent)
- Text retrieval: Dense (BGE) + Sparse (BM42) with RRF fusion
- Image retrieval: CLIP embeddings for visual search
- `_calculate_coverage()`: Calculates paper and section coverage stats
- `_is_evidence_sufficient()`: Determines if evidence meets thresholds

**Flow**:
```
RetrievalEvent â†’ Dense Query Embedding â†’ Sparse Query Embedding â†’ 
â”œâ”€ Hybrid Search (Text) â†’ EvidenceChunk[]
â””â”€ CLIP Search (Images) â†’ ImageEvidence[] â†’ Coverage Check â†’ AnalysisEvent
```

**Dependencies**:
- `app.services.embeddings`: BGE embeddings
- `app.services.clip_embedding`: CLIP embeddings
- `app.db.qdrant_client`: Qdrant search operations
- `app.config`: Feature flags (enable_multimodal, enable_hybrid_search)

---

#### `analysis_synthesis.py` (Agent 3: The Reasoner)
**Purpose**: Performs all reasoning and synthesis to generate cited answers.

**Key Components**:
- `AnalysisSynthesisAgent`: Synthesis agent with intent-aware routing
- `process()`: Routes to appropriate synthesis method based on intent
- `_synthesize_summary()`: Brief/concise summary generation
- `_compare_papers()`: Cross-paper comparison
- `_identify_gaps()`: Research gaps and limitations analysis
- `_is_brief_summary_requested()`: Detects verbosity hints (rule-based)
- `_estimate_confidence()`: Heuristic confidence scoring

**Intent Routing**:
- SUMMARY â†’ `_synthesize_summary()` (with brief_mode detection)
- COMPARISON â†’ `_compare_papers()`
- RESEARCH_GAPS â†’ `_identify_gaps()`

**Dependencies**:
- `app.services.llm_service`: Groq LLM for synthesis
- `app.models.events`: AnalysisEvent, StopEvent, HumanReviewEvent

---

### 2. Workflow Layer (`backend/app/workflows/`)

#### `research_workflow.py`
**Purpose**: Native LlamaIndex Workflow orchestrating all three agents.

**Key Components**:
- `ResearchWorkflow`: LlamaIndex Workflow class with @step decorators
- `orchestrate_query()`: Step 1 - Consumes StartEvent, produces RetrievalEvent
- `retrieve_evidence()`: Step 2 - Consumes RetrievalEvent, produces AnalysisEvent
- `analyze_and_synthesize()`: Step 3 - Consumes AnalysisEvent, produces StopEvent
- `handle_human_review()`: Step 4 - Handles HITL escalation
- `execute_workflow()`: Convenience function for simple execution

**Event Flow**:
```
StartEvent 
    â†“
@step orchestrate_query â†’ RetrievalEvent
    â†“
@step retrieve_evidence â†’ AnalysisEvent | HumanReviewEvent
    â†“
@step analyze_and_synthesize â†’ StopEvent | HumanReviewEvent
    â†“
@step handle_human_review â†’ StopEvent
```

**Dependencies**:
- `llama_index.core.workflow`: Workflow, StartEvent, StopEvent, step
- `app.agents.*`: All three agent classes
- `app.models.events`: Custom event types

---

### 3. API Routes (`backend/app/api/routes/`)

#### `query.py`
**Endpoints**:
- `POST /api/query` - Full RAG query with citations and images
- `POST /api/query/simple` - Simplified query (question + top_k only)
- `GET /api/query/examples` - Returns example queries by category
- `GET /api/query/health` - Query engine health check

**Features**:
- Supports dense, sparse, and hybrid search modes
- Returns text sources + related images
- Langfuse tracing on all endpoints

---

#### `search.py`
**Endpoints**:
- `POST /api/search` - Dense-only vector search
- `POST /api/search/hybrid` - Hybrid search (Dense + BM42 + RRF)
- `GET /api/corpus/stats` - Corpus statistics with hybrid info

**HybridSearchRequest**:
```python
{
    "query": "What is LoRA rank?",
    "top_k": 5,
    "sections": ["Methods", "Results"]  # Optional section filter
}
```

---

#### `upload.py`
**Endpoints**:
- `POST /api/upload` - Upload PDF with auto-processing (background task)
- `GET /api/upload/status/{filename}` - Check processing status
- `GET /api/upload/list` - List all PDFs in corpus

**Background Processing** (`process_pdf`):
1. Parse PDF with section detection
2. Chunk text with metadata
3. Generate dense embeddings (BGE)
4. Generate sparse embeddings (BM42) if hybrid enabled
5. Extract images and generate CLIP embeddings if multimodal enabled
6. Insert into Qdrant (text + image collections)

---

#### `sessions.py`
**Endpoints**:
- `GET /api/sessions` - List all chat sessions
- `POST /api/sessions` - Create new session
- `GET /api/sessions/{session_id}` - Get session with message history
- `DELETE /api/sessions/{session_id}` - Delete session
- `PATCH /api/sessions/{session_id}` - Rename session
- `POST /api/sessions/{session_id}/query` - Query within a session

**Session Flow**:
```
1. Save user message to MongoDB
2. Run RAG query (existing logic)
3. Save assistant response to MongoDB
4. Return response with session context
```

---

#### `image_search.py`
**Endpoints**:
- `POST /api/image-search` - Search images using text query (CLIP)
- `GET /api/image-stats` - Get image collection statistics

**Request**:
```python
{
    "query": "show me LoRA architecture diagram",
    "top_k": 3,
    "min_score": 0.15
}
```

---

#### `images.py`
**Endpoints**:
- `GET /api/image/{paper_title}/{page_number}/{image_index}` - Serve image from PDF
- `GET /api/image-by-id/{image_id}` - Serve image by Qdrant ID

**Image Extraction Strategy**:
- On-demand extraction from PDF using PyMuPDF
- No persistent image storage (diskless)
- Metadata stored in Qdrant, images extracted at request time

---

### 4. Database Layer (`backend/app/db/`)

#### `qdrant_client.py`
**Purpose**: Qdrant vector database operations with hybrid search support.

**Key Classes**:
- `QdrantService`: Main service class

**Methods**:
- `create_collection()`: Creates hybrid collection (dense + sparse vectors)
- `create_image_collection()`: Creates image collection (CLIP vectors)
- `insert_chunks()`: Inserts text chunks with hybrid embeddings
- `insert_images()`: Inserts image metadata with CLIP embeddings
- `search()`: Backward-compatible dense-only search
- `search_with_filter()`: Hybrid search with section filtering
- `_hybrid_search()`: Internal hybrid search with prefetch
- `_rrf_fusion()`: Reciprocal Rank Fusion algorithm
- `search_images()`: CLIP-based image search
- `count()`, `count_images()`: Collection statistics

**Hybrid Search Algorithm**:
```python
1. Dense search â†’ top-K results (semantic)
2. Sparse search â†’ top-K results (keyword)
3. RRF fusion: score = Î£ 1/(k + rank_i)
4. Return merged results
```

**Configuration**:
- Collection name: `research_papers_hybrid`
- Dense vector: `text-dense` (768-dim, COSINE)
- Sparse vector: `sparse` (BM42)
- Image collection: `research_papers_images` (512-dim, COSINE)

---

#### `mongo_client.py`
**Purpose**: MongoDB connection singleton for session persistence.

**Key Functions**:
- `get_mongo_db()`: Returns singleton MongoDB database instance

**Usage**:
- Session metadata storage
- Chat message history (via LlamaIndex MongoChatStore)
- Direct pymongo access for custom queries

---

### 5. Model Layer (`backend/app/models/`)

#### `events.py`
**Purpose**: Workflow event definitions for LlamaIndex Workflow.

**Event Types**:
- `IntentType` (Enum): SUMMARY, COMPARISON, RESEARCH_GAPS
- `StartEvent`: Initial user question with session_id
- `RetrievalEvent`: Triggers evidence retrieval with intent and sections
- `EvidenceChunk`: Retrieved text with metadata
- `ImageEvidence`: Retrieved image with metadata
- `AnalysisEvent`: Triggers analysis with chunks + images
- `HumanReviewEvent`: Requests human intervention
- `StopEvent`: Final answer or refusal

---

#### `paper.py`
**Purpose**: Research paper data models.

**Classes**:
- `PaperMetadata`: Title, authors, year, num_pages, num_images
- `Section`: section_id, title, content, page_start, page_end
- `ParsedPaper`: paper_id, filename, metadata, sections[], raw_text

---

#### `chunk.py`
**Purpose**: Text chunk and search result models.

**Classes**:
- `ChunkMetadata`: paper_id, paper_title, section_title, page_start, page_end
- `Chunk`: chunk_id, text, metadata, embedding, sparse_embedding
- `SearchResult`: text, score, metadata
- `SearchRequest` / `SearchResponse`: API models

---

#### `image.py`
**Purpose**: Image metadata models for multimodal RAG.

**Classes**:
- `ImageMetadata`: image_id, paper_id, paper_title, page_number, caption, image_type
- `ExtractedImage`: Wrapper for extracted image with metadata
- `ImageSearchResult`: Result from image search with score
- `ImageSearchRequest` / `ImageSearchResponse`: API models

---

#### `session.py`
**Purpose**: Chat session and message models.

**Classes**:
- `SessionCreate` / `SessionRename`: Request models
- `SessionMessage`: role, content, sources, images, search_mode, timestamp
- `SessionInfo`: session_id, title, created_at, updated_at, message_count
- `SessionDetail`: Full session with messages[]
- `SessionQueryRequest`: question, similarity_top_k, search_mode

---

#### `query.py`
**Purpose**: Query API request/response models.

**Classes**:
- `QueryRequest`: question, similarity_top_k, response_mode, search_mode
- `SourceInfo`: paper_id, paper_title, section_title, pages, score, text
- `ImageInfo`: image_id, paper_title, page_number, caption, image_type, score
- `QueryResponse`: question, answer, sources[], images[], num_sources, response_mode

---

### 6. Service Layer (`backend/app/services/`)

#### `llm_service.py`
**Purpose**: Groq LLM integration for LlamaIndex.

**Key Function**:
- `get_llm()`: Returns LlamaIndex Groq LLM instance

**Configuration**:
- Model: `openai/gpt-oss-120b` (or other Groq models)
- Temperature: 0.1 (deterministic)
- API: Groq (fast, free inference)

---

#### `embeddings.py`
**Purpose**: Dense and sparse embedding services.

**Classes**:
- `EmbeddingService`: Dense embeddings (BGE)
  - Model: `BAAI/bge-base-en-v1.5`
  - Dimension: 768
  - Device: CPU (configurable)
  
- `SparseEmbeddingService`: Sparse embeddings (BM42)
  - Model: `Qdrant/bm42-all-minilm-l6-v2-attentions`
  - Library: `fastembed`
  - SparseVector format for Qdrant

**Functions**:
- `get_embedding_service()`: Returns singleton dense service
- `get_sparse_embedding_service()`: Returns singleton sparse service
- `get_llamaindex_embed_model()`: Returns LlamaIndex-compatible embed model

---

#### `clip_embedding.py`
**Purpose**: CLIP embeddings for image-text multimodal search.

**Class**: `CLIPEmbeddingService`

**Capabilities**:
- `generate_text_embedding(text)`: Text â†’ 512-dim vector
- `generate_image_embedding(pil_image)`: Image â†’ 512-dim vector
- `generate_image_embeddings_batch(images)`: Batched image embedding

**Configuration**:
- Model: `ViT-B/32`
- Dimension: 512
- Device: CUDA if available, else CPU
- Normalization: Cosine similarity

---

#### `pdf_parser.py`
**Purpose**: Section-aware PDF parsing for research papers.

**Classes**:
- `SectionAwarePDFParser`: Recommended parser with section detection
- `LlamaIndexPDFParser`: Basic LlamaIndex parsing
- `AdvancedPDFParser`: Enhanced parsing with layout understanding

**Section Detection**:
- 13 canonical sections: Abstract, Introduction, Related Work, Methods, Experiments, Results, Discussion, Limitations, Future Work, Conclusion, References, Appendix, Unknown
- Pattern matching with regex
- Noise filtering (tables, figures, OCR artifacts)
- Normalization to strict taxonomy

**Key Methods**:
- `parse()`: Returns ParsedPaper with detected sections
- `_detect_sections()`: Finds section headers across pages
- `_match_section_header()`: Matches line against patterns
- `_normalize_section_title()`: Maps to canonical names
- `_is_noise()`: Filters false positives

---

#### `chunking.py`
**Purpose**: Smart text chunking with section awareness.

**Classes**:
- `LlamaIndexChunker` / `Chunker`: Main chunking service
- `SemanticChunker`: Semantic-based chunking (future)
- `SentenceWindowChunker`: Overlapping windows (future)

**Key Methods**:
- `chunk_paper(paper)`: Chunks entire paper with metadata
- `chunk_with_metadata()`: Section-aware chunking
- `_chunk_full_text()`: Fallback full-text chunking

**Configuration**:
- Chunk size: 1000 characters
- Overlap: 200 characters
- Splitter: LlamaIndex SentenceSplitter

---

#### `image_extraction.py`
**Purpose**: Extract images from PDFs (in-memory, stateless).

**Class**: `PDFImageExtractor`

**Strategy**:
- IN-MEMORY ONLY: No disk persistence
- Extract as PIL Images
- Generate embeddings immediately
- Store only metadata + embeddings in Qdrant

**Methods**:
- `extract_images_from_pdf()`: Returns List[(PIL_Image, ImageMetadata)]
- `_classify_image_type()`: Heuristic classification (figure/chart/diagram/table)

**Filtering**:
- Min width: 100px
- Min height: 100px
- CMYK to RGB conversion

---

#### `intent_classifier.py`
**Purpose**: Rule-based intent classification for section-aware retrieval.

**Class**: `IntentClassifier`

**Design Principles**:
- NO ML, NO embeddings, NO LLM
- Pure keyword-based matching
- Priority-based conflict resolution

**Intent Mappings**:
| Intent | Keywords | Sections |
|--------|----------|----------|
| limitations | limitation, drawback, weakness... | Discussion, Limitations |
| methodology | method, approach, algorithm... | Methods |
| experiments | experiment, benchmark, dataset... | Experiments, Results |
| summary | summary, overview, what is... | Abstract, Introduction |
| comparison | compare, versus, vs... | Results, Experiments |

**Priority Order**: citation(100) > limitations(90) > future_work(85) > ... > general(10)

**Methods**:
- `classify(query)`: Returns IntentResult
- `get_qdrant_filter(intent_result)`: Builds Qdrant filter

---

#### `hitl_gate.py`
**Purpose**: Human-in-the-loop quality gate.

**Function**: `evaluate_hitl_gate()`

**Trigger Conditions** (ANY of):
- `retrieved_chunks_count < 2`
- `intent_confidence < 0.6`
- `paper_coverage == 0`

**Response**:
- `HITLDecision`: should_proceed, requires_human_review, reason, stats
- `format_hitl_response()`: Formats for API response

**Design**:
- Deterministic, rule-based
- NO ML or LLM involvement
- Clear thresholds for transparency

---

#### `guardrails_service.py`
**Purpose**: Production-grade output validation using Guardrails AI.

**Class**: `GuardrailsService`

**Validation Pipeline**:
1. **Schema Validation**: Pydantic model enforcement with auto-retry
2. **Citation Grounding**: Verify citations exist in retrieved chunks
3. **Hallucination Detection**: Heuristic pattern matching
4. **Confidence Check**: Final threshold verification

**Pydantic Models**:
- `Citation`: paper_title, page_start, page_end with validation
- `ValidatedAnswer`: answer, citations[], confidence, refused
- `HITLGuardrailResponse`: Human review trigger format

**Methods**:
- `validate_and_enforce()`: Main validation entry point
- `_validate_schema()`: Guardrails AI Pydantic validation
- `_validate_citation_grounding()`: Rule-based citation check
- `_check_hallucinations()`: Heuristic hallucination detection
- `_create_hitl_response()`: Formats HITL escalation

**Configuration**:
- Max retries: 1
- Confidence penalty for bad citations: 0.15 per citation
- Minimum final confidence: 0.5

---

#### `query_engine.py`
**Purpose**: Intelligent RAG query engine with multimodal support.

**Class**: `IntelligentQueryEngine`

**Components**:
- Qdrant client for vector operations
- VectorStoreIndex for LlamaIndex integration
- CLIP service for image retrieval

**Methods**:
- `query()`: Main query method with search_mode support
- `_query_with_mode()`: Direct Qdrant query for hybrid/sparse modes
- `_get_related_images()`: CLIP-based image retrieval

**Search Modes**:
- `dense`: BGE embeddings only (LlamaIndex)
- `sparse`: BM42 sparse embeddings only
- `hybrid`: RRF fusion of dense + sparse

---

#### `session_service.py`
**Purpose**: ChatGPT-style session management with MongoDB.

**Class**: `SessionService`

**Features**:
- Session CRUD (Create, Read, Update, Delete)
- Chat history persistence via MongoChatStore
- Message metadata (sources, images, search_mode)
- Auto-title generation from first message

**Methods**:
- `create_session()`, `list_sessions()`, `get_session()`, `delete_session()`, `rename_session()`
- `add_user_message()`, `add_assistant_message()`

**Storage**:
- Session metadata: pymongo (sessions collection)
- Chat messages: LlamaIndex MongoChatStore

---

#### `langfuse_utils.py`
**Purpose**: Central Langfuse observability utilities.

**Functions**:
- `get_langfuse()`: Returns singleton Langfuse client
- `flush_langfuse()`: Flushes pending traces on shutdown

**Integration**:
- Used by @observe decorator throughout codebase
- Lifecycle managed in FastAPI lifespan
- Graceful handling of missing configuration

---

### 7. Configuration (`backend/app/`)

#### `config.py`
**Purpose**: Centralized configuration using Pydantic Settings.

**Class**: `Settings` (BaseSettings)

**Configuration Categories**:

**Groq LLM**:
- `groq_api_key`: API key for Groq inference
- `llm_model`: Model name (default: `openai/gpt-oss-120b`)
- `llm_temperature`: Sampling temperature (0.1)

**Qdrant**:
- `qdrant_host`, `qdrant_port`: Connection settings
- `qdrant_collection_name`: Text collection
- `qdrant_image_collection_name`: Image collection

**Embeddings**:
- `embedding_model`: Dense model (BAAI/bge-base-en-v1.5)
- `embedding_dim`: 768
- `sparse_embedding_model`: BM42 model
- `enable_hybrid_search`: Toggle sparse embeddings
- `rrf_k`: Reciprocal Rank Fusion constant (60)
- `dense_weight`, `sparse_weight`: Fusion weights (0.5 each)

**CLIP**:
- `clip_model_name`: ViT-B/32
- `clip_embedding_dim`: 512
- `enable_multimodal`: Toggle image extraction
- `min_image_width`, `min_image_height`: Size filters

**Chunking**:
- `chunk_size`: 1000 characters
- `chunk_overlap`: 200 characters
- `similarity_top_k`: Default retrieval count (5)

**Workflow**:
- `enable_guardrails`: Toggle validation
- `confidence_threshold`: Minimum confidence (0.5)

**Langfuse**:
- `langfuse_public_key`, `langfuse_secret_key`: Auth
- `langfuse_host`: Server URL (http://localhost:3000)
- `enable_langfuse`: Toggle tracing

**MongoDB**:
- `mongodb_uri`: Connection string
- `mongodb_db_name`: Database name (research_paper_intel)

**Paths**:
- `corpus_dir`: PDF storage location (../corpus)

---

#### `main.py`
**Purpose**: FastAPI application entry point.

**Features**:
- Lifespan management (startup/shutdown)
- Langfuse LlamaIndex callback initialization
- Router registration
- Health check endpoints

**Lifespan**:
```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup: Initialize Langfuse
    get_langfuse()
    yield
    # Shutdown: Flush traces
    flush_langfuse()
```

**Routers**:
- `/api` - search, query, upload, sessions, image_search, images

**Endpoints**:
- `GET /` - Root message
- `GET /health` - Health check with agent count

---

### 8. Frontend (`frontend/`)

#### `app.py`
**Purpose**: Streamlit ChatGPT-style web interface.

**Features**:
- Dark theme UI
- Session management sidebar
- PDF upload with progress tracking
- Chat history display
- Source and image rendering
- Search mode selector (hybrid/dense/sparse)
- Top-k slider (3-10)

**Layout**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sidebar              â”‚  Main Content    â”‚
â”‚  â”œâ”€ New Chat Button   â”‚  â”œâ”€ Header       â”‚
â”‚  â”œâ”€ Session List      â”‚  â”œâ”€ Messages     â”‚
â”‚  â”œâ”€ PDF Upload        â”‚  â”œâ”€ Images       â”‚
â”‚  â”œâ”€ Stats             â”‚  â”œâ”€ Sources      â”‚
â”‚  â””â”€ Health Status     â”‚  â””â”€ Chat Input   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Session Flow**:
1. User selects/creates session
2. Types question in chat input
3. Frontend POSTs to `/api/sessions/{id}/query`
4. Displays answer + images + sources
5. Saves to session history

---

### 9. Standalone Scripts

#### `build_corpus.py`
**Purpose**: CLI tool to build multimodal corpus from PDFs.

**Pipeline**:
1. Scan corpus/ directory for PDFs
2. Parse each PDF with SectionAwarePDFParser
3. Chunk with metadata preservation
4. Generate dense embeddings (BGE)
5. Generate sparse embeddings (BM42) if hybrid enabled
6. Extract images and generate CLIP embeddings if multimodal enabled
7. Insert text chunks and image embeddings into Qdrant

**Usage**:
```bash
python build_corpus.py
```

**Output**:
- Summary of processed papers
- Count of text chunks and images
- Vector database statistics

---

#### `interactive_query.py`
**Purpose**: CLI interface for asking questions.

**Features**:
- Interactive terminal interface
- 3-agent workflow execution
- Displays answer, metadata, and citations
- Session continuity support

**Usage**:
```bash
python interactive_query.py
# Type questions at prompt
# Type 'quit' to exit
```

**Output**:
```
ðŸ’¡ ANSWER:
[Generated answer text]

ðŸ“Š METADATA:
   Intent: methodology
   Confidence: 0.85

ðŸ“š CITATIONS (3):
   1. Paper Title
      Pages: 3-4
```

---

## ðŸ”€ Data Flow Diagrams

### Ingestion Pipeline Flow

```mermaid
sequenceDiagram
    participant User
    participant UploadAPI as Upload API
    participant Parser as PDF Parser
    participant Chunker as Chunking Service
    participant Embeddings as Embedding Services
    participant ImageExt as Image Extractor
    participant Qdrant as Qdrant DB

    User->>UploadAPI: POST /api/upload (PDF file)
    UploadAPI->>UploadAPI: Save to corpus/ folder
    UploadAPI->>Parser: Trigger background processing
    
    par Text Processing
        Parser->>Parser: Parse PDF with section detection
        Parser->>Chunker: Extract sections
        Chunker->>Chunker: Create chunks (1000 chars, 200 overlap)
        Chunker->>Embeddings: Generate embeddings
        Embeddings->>Embeddings: Dense (BGE-768)
        Embeddings->>Embeddings: Sparse (BM42)
        Embeddings->>Qdrant: Insert text chunks
    and Image Processing
        Parser->>ImageExt: Extract images
        ImageExt->>ImageExt: Filter (min 100x100)
        ImageExt->>Embeddings: Generate CLIP embeddings
        Embeddings->>Qdrant: Insert image embeddings
    end
    
    Qdrant-->>UploadAPI: Confirm storage
    UploadAPI-->>User: Return status + processing ID
```

---

### Query Pipeline Flow

```mermaid
sequenceDiagram
    participant User
    participant API as Query API
    participant Workflow as ResearchWorkflow
    participant Agent1 as QueryOrchestrator
    participant Agent2 as EvidenceRetrieval
    participant HITL as HITL Gate
    participant Agent3 as AnalysisSynthesis
    participant Guardrails as Guardrails AI
    participant Qdrant as Qdrant DB
    participant LLM as Groq LLM

    User->>API: POST /api/query (question)
    API->>Workflow: Execute workflow
    
    Workflow->>Agent1: Step 1: Orchestrate
    Agent1->>LLM: Classify intent
    LLM-->>Agent1: Intent + Sections
    Agent1-->>Workflow: RetrievalEvent
    
    Workflow->>Agent2: Step 2: Retrieve Evidence
    Agent2->>Qdrant: Hybrid search (text)
    Agent2->>Qdrant: CLIP search (images)
    Qdrant-->>Agent2: Text chunks + Images
    Agent2->>HITL: Check coverage
    HITL-->>Agent2: Proceed/Block
    Agent2-->>Workflow: AnalysisEvent
    
    Workflow->>Agent3: Step 3: Analyze
    Agent3->>LLM: Synthesize answer
    LLM-->>Agent3: Generated answer
    Agent3-->>Workflow: StopEvent
    
    Workflow->>Guardrails: Validate output
    Guardrails->>Guardrails: Schema check
    Guardrails->>Guardrails: Citation grounding
    Guardrails->>Guardrails: Hallucination check
    Guardrails-->>Workflow: Validated result
    
    Workflow-->>API: Final result
    API-->>User: Answer + Citations + Images
```

---

### Hybrid Search Algorithm Flow

```mermaid
graph LR
    A[Query Text] --> B[Dense Embedding<br/>BGE-768]
    A --> C[Sparse Embedding<br/>BM42]
    
    B --> D[Dense Search<br/>Qdrant]
    C --> E[Sparse Search<br/>Qdrant]
    
    D --> F[Dense Results<br/>Rank 1-N]
    E --> G[Sparse Results<br/>Rank 1-N]
    
    F --> H[RRF Fusion<br/>k=60]
    G --> H
    
    H --> I[RRF Score Calculation<br/>score = 1/(k+rank)]
    I --> J[Sort by Score<br/>Descending]
    J --> K[Top-K Results<br/>Merged & Ranked]
```

---

## ðŸ› ï¸ Tech Stack Deep Dive

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Framework** | LlamaIndex | 0.12.x | RAG orchestration & workflow |
| **API Framework** | FastAPI | 0.115.x | REST API with OpenAPI docs |
| **LLM Provider** | Groq | - | Fast inference (GPT-4 class) |
| **Dense Embeddings** | HuggingFace (BGE) | bge-base-en-v1.5 | 768-dim semantic vectors |
| **Sparse Embeddings** | FastEmbed (BM42) | - | Keyword matching vectors |
| **Image Embeddings** | OpenAI CLIP | ViT-B/32 | 512-dim cross-modal vectors |
| **Vector Database** | Qdrant | 1.13.x | Hybrid vector storage |
| **Session Storage** | MongoDB | 7.x | Chat history persistence |
| **PDF Parsing** | PyMuPDF (fitz) | - | PDF text & image extraction |
| **Validation** | Guardrails AI | 0.6.x | Output schema enforcement |
| **Observability** | Langfuse | 2.x | Distributed tracing |
| **Frontend** | Streamlit | 1.41.x | ChatGPT-style UI |
| **Python** | Python | 3.10+ | Runtime environment |

---

## âœ… Current Feature Status

### Week 1: Core Infrastructure âœ…
| Component | Technology | Status |
|-----------|------------|--------|
| PDF Parsing | PyMuPDF + LlamaIndex | âœ… Done |
| Section Detection | Pattern matching | âœ… Done |
| Chunking | SentenceSplitter | âœ… Done |
| Dense Embeddings | BGE-768 | âœ… Done |
| Vector DB | Qdrant | âœ… Done |
| API Framework | FastAPI | âœ… Done |

### Week 2: Query Engine âœ…
| Component | Technology | Status |
|-----------|------------|--------|
| LLM Integration | Groq | âœ… Done |
| RAG Pipeline | LlamaIndex | âœ… Done |
| Query API | REST endpoints | âœ… Done |
| Hybrid Search | Dense + Sparse | âœ… Done |

### Week 3: Multi-Agent Workflow âœ…
| Component | Technology | Status |
|-----------|------------|--------|
| Query Orchestrator | Intent classification | âœ… Done |
| Evidence Retrieval | Hybrid + CLIP | âœ… Done |
| Analysis Synthesis | LLM reasoning | âœ… Done |
| HITL Gate | Rule-based | âœ… Done |
| LlamaIndex Workflow | @step decorators | âœ… Done |

### Week 4: Quality & Validation âœ…
| Component | Technology | Status |
|-----------|------------|--------|
| Guardrails AI | Pydantic validation | âœ… Done |
| Citation Grounding | Rule-based | âœ… Done |
| Hallucination Detection | Heuristics | âœ… Done |
| Auto-Retry | Max 1 retry | âœ… Done |

### Week 5: Multimodal Support âœ…
| Component | Technology | Status |
|-----------|------------|--------|
| Image Extraction | PyMuPDF | âœ… Done |
| CLIP Embeddings | ViT-B/32 | âœ… Done |
| Image Search | Text-to-image | âœ… Done |
| Image Serving | On-demand | âœ… Done |

### Week 6: Session Management âœ…
| Component | Technology | Status |
|-----------|------------|--------|
| Chat Sessions | MongoDB | âœ… Done |
| Session History | LlamaIndex ChatStore | âœ… Done |
| Frontend UI | Streamlit | âœ… Done |
| Message Persistence | Full | âœ… Done |

### Week 7: Observability âœ…
| Component | Technology | Status |
|-----------|------------|--------|
| Langfuse Tracing | @observe decorators | âœ… Done |
| LlamaIndex Callback | CallbackManager | âœ… Done |
| Lifecycle Management | Startup/Flush | âœ… Done |
| Workflow Tracing | All steps | âœ… Done |

---

## ðŸš€ Quick Start Guide

### Prerequisites
- Python 3.10+
- Docker & Docker Compose
- Groq API key (free at https://console.groq.com)

### 1. Clone and Setup

```bash
git clone <repository-url>
cd research-paper-intelligence-system

# Create virtual environment
python -m venv venv_clean
source venv_clean/bin/activate  # Linux/Mac
# or: .\venv_clean\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Environment Configuration

Create `.env` file:

```env
# Required
GROQ_API_KEY=your_groq_api_key_here

# Optional (with defaults)
QDRANT_HOST=localhost
QDRANT_PORT=6333
MONGODB_URI=mongodb://localhost:27017

# Langfuse (optional, for tracing)
LANGFUSE_PUBLIC_KEY=your_public_key
LANGFUSE_SECRET_KEY=your_secret_key
LANGFUSE_HOST=http://localhost:3000
ENABLE_LANGFUSE=true
```

### 3. Start Infrastructure

```bash
# Start Qdrant and MongoDB
docker-compose up -d

# Verify services
curl http://localhost:6333/collections
mongosh --eval "db.version()"
```

### 4. Build Corpus

```bash
# Place PDFs in corpus/ folder, then:
python build_corpus.py
```

### 5. Run the System

**Option A: FastAPI Backend**
```bash
cd backend
uvicorn app.main:app --reload --port 8000

# API docs: http://localhost:8000/docs
```

**Option B: Streamlit Frontend**
```bash
# In another terminal
streamlit run frontend/app.py --server.port 8501

# UI: http://localhost:8501
```

**Option C: Interactive CLI**
```bash
python interactive_query.py
```

---

## ðŸ“š API Reference

### Query Endpoints

#### Intelligent Query
```bash
POST /api/query
Content-Type: application/json

{
  "question": "What are the limitations of LoRA?",
  "similarity_top_k": 5,
  "search_mode": "hybrid"
}
```

**Response**:
```json
{
  "question": "What are the limitations of LoRA?",
  "answer": "LoRA has several limitations...",
  "sources": [
    {
      "paper_id": "uuid",
      "paper_title": "LORA: LOW-RANK ADAPTATION",
      "section_title": "Limitations",
      "page_start": 8,
      "page_end": 9,
      "score": 0.85
    }
  ],
  "images": [
    {
      "image_id": "uuid",
      "paper_title": "LORA Paper",
      "page_number": 5,
      "score": 0.72
    }
  ],
  "num_sources": 3
}
```

### Search Endpoints

#### Hybrid Search
```bash
POST /api/search/hybrid
Content-Type: application/json

{
  "query": "LoRA architecture",
  "top_k": 5,
  "sections": ["Methods", "Results"]
}
```

### Session Endpoints

#### Create Session
```bash
POST /api/sessions
Content-Type: application/json

{
  "title": "LoRA Research"
}
```

#### Query in Session
```bash
POST /api/sessions/{session_id}/query
Content-Type: application/json

{
  "question": "How does LoRA work?",
  "similarity_top_k": 5,
  "search_mode": "hybrid"
}
```

---

## ðŸ§ª Testing Examples

### Test Intent Classification
```bash
# Methodology intent
curl -X POST http://localhost:8000/api/query \
  -d '{"question": "How does the training work?"}'

# Limitations intent  
curl -X POST http://localhost:8000/api/query \
  -d '{"question": "What are the limitations?"}'

# Brief summary
curl -X POST http://localhost:8000/api/query \
  -d '{"question": "Give me a brief summary"}'
```

### Test Hybrid Search
```bash
curl -X POST http://localhost:8000/api/search/hybrid \
  -d '{"query": "What is LoRA rank?", "top_k": 3}'
```

### Test Image Search
```bash
curl -X POST http://localhost:8000/api/image-search \
  -d '{"query": "show me architecture diagram", "top_k": 3}'
```

---

## âš™ï¸ Advanced Configuration

### Hybrid Search Weights
Edit `backend/app/config.py`:

```python
# RRF Fusion Parameters
rrf_k: int = 60  # Reciprocal Rank Fusion constant
dense_weight: float = 0.5  # Dense vector weight
sparse_weight: float = 0.5  # Sparse vector weight
```

### Chunking Parameters
```python
chunk_size: int = 1000      # Characters per chunk
chunk_overlap: int = 200    # Overlap between chunks
```

### HITL Thresholds
Edit `backend/app/services/hitl_gate.py`:

```python
MIN_CHUNKS_REQUIRED = 2
MIN_INTENT_CONFIDENCE = 0.6
```

### Guardrails Strictness
Edit `backend/app/services/guardrails_service.py`:

```python
MAX_RETRIES = 1
confidence_threshold = 0.5  # Minimum final confidence
```

---

## ðŸ—ºï¸ Roadmap

- [x] **Week 1**: Core Infrastructure
- [x] **Week 2**: RAG Query Engine
- [x] **Week 3**: Multi-Agent Workflow
- [x] **Week 4**: Guardrails AI
- [x] **Week 5**: BM42 Hybrid Search
- [x] **Week 6**: Multimodal Support
- [x] **Week 7**: Session Management & Observability
- [ ] **Week 8**: Cloud Deployment (AWS/GCP)
- [ ] **Week 9**: Advanced Analytics Dashboard
- [ ] **Week 10**: Multi-tenant Support

---

## ðŸ¤ Contributing

This is a research project demonstrating production-grade RAG implementation. Feel free to fork and extend!

---

## ðŸ“ License

MIT License

---

## ðŸ™ Acknowledgments

- **LlamaIndex**: RAG framework and workflow engine
- **Qdrant**: Hybrid vector database
- **Groq**: Fast LLM inference
- **Langfuse**: Observability and tracing
- **Guardrails AI**: Output validation
- **Streamlit**: Interactive frontend

---

Built with â¤ï¸ using LlamaIndex, Qdrant, Groq, CLIP, and Langfuse
