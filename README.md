# ğŸ”¬ Research Paper Intelligence System

A production-ready RAG (Retrieval-Augmented Generation) pipeline for processing and searching research papers using **LlamaIndex**, **HuggingFace embeddings**, and **Qdrant** vector database.

## ğŸ¯ Project Overview

This system ingests PDF research papers, chunks them intelligently, generates embeddings, and stores them in a vector database for semantic search.

```mermaid
flowchart LR
    A[PDFs<br/>corpus/] -->|LlamaIndex<br/>SimpleDirectoryReader| B(LlamaIndex<br/>Smart Chunking)
    B -->|SentenceSplitter| C{Vector Embeddings<br/>768 dimensions}
    C -->|HuggingFace<br/>BGE-base-en-v1.5| D[Qdrant<br/>Vector DB]
```

## âœ… Current Progress

### Week 1: Core Infrastructure (Completed)

| Component | Technology | Status |
|-----------|------------|--------|
| **PDF Parsing** | LlamaIndex `SimpleDirectoryReader` | âœ… Done |
| **Chunking** | LlamaIndex `SentenceSplitter` | âœ… Done |
| **Embeddings** | `BAAI/bge-base-en-v1.5` (768 dim) | âœ… Done |
| **Vector DB** | Qdrant | âœ… Done |
| **API Framework** | FastAPI | âœ… Done |

### Features Implemented

- ğŸ“„ **PDF Processing**: Automatic text extraction using LlamaIndex
- âœ‚ï¸ **Smart Chunking**: Sentence-aware splitting with configurable overlap
- ğŸ§  **Free Embeddings**: Local HuggingFace model (no API costs!)
- ğŸ—„ï¸ **Vector Storage**: Qdrant for fast similarity search
- ğŸ” **Search API**: FastAPI endpoints for querying

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Framework** | LlamaIndex | RAG orchestration |
| **Embeddings** | BAAI/bge-base-en-v1.5 | Text â†’ 768-dim vectors |
| **Vector DB** | Qdrant | Similarity search |
| **PDF Reader** | LlamaIndex + PyMuPDF | Document ingestion |
| **API** | FastAPI | REST endpoints |
| **Chunking** | SentenceSplitter | Semantic text splitting |

## ğŸ“ Project Structure

```
research-paper-intelligence-system/
â”œâ”€â”€ corpus/                     # Put your PDFs here
â”‚   â”œâ”€â”€ paper1.pdf
â”‚   â””â”€â”€ paper2.pdf
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ api/               # FastAPI routes
â”‚       â”‚   â””â”€â”€ routes/
â”‚       â”‚       â””â”€â”€ search.py
â”‚       â”œâ”€â”€ db/
â”‚       â”‚   â””â”€â”€ qdrant_client.py   # Qdrant integration
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ paper.py          # Paper data models
â”‚       â”‚   â””â”€â”€ chunk.py          # Chunk data models
â”‚       â”œâ”€â”€ services/
â”‚       â”‚   â”œâ”€â”€ pdf_parser.py     # LlamaIndex PDF parsing
â”‚       â”‚   â”œâ”€â”€ chunking.py       # LlamaIndex SentenceSplitter
â”‚       â”‚   â””â”€â”€ embeddings.py     # HuggingFace embeddings
â”‚       â”œâ”€â”€ config.py             # Settings & configuration
â”‚       â””â”€â”€ main.py               # FastAPI app
â”œâ”€â”€ build_corpus.py            # Main ingestion script
â”œâ”€â”€ docker-compose.yml         # Qdrant container
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.10+
- Docker (for Qdrant)

### 2. Setup

```bash
# Clone and navigate
cd research-paper-intelligence-system

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 3. Start Qdrant

```bash
docker-compose up -d
```

### 4. Add PDFs

Place your research papers in the `corpus/` directory:
```
corpus/
â”œâ”€â”€ paper1.pdf
â””â”€â”€ paper2.pdf
```

### 5. Build Corpus

```bash
python build_corpus.py
```

This will:
1. Parse all PDFs using LlamaIndex
2. Chunk text with SentenceSplitter
3. Generate embeddings with BAAI/bge-base-en-v1.5
4. Store in Qdrant vector database

### 6. Start API

```bash
cd backend
uvicorn app.main:app --reload
```

Visit: http://localhost:8000/docs

## âš™ï¸ Configuration

Edit `backend/app/config.py`:

```python
# Embedding Model
embedding_model: str = "BAAI/bge-base-en-v1.5"
embedding_dim: int = 768

# Chunking
chunk_size: int = 1000      # Max tokens per chunk
chunk_overlap: int = 200    # Overlap between chunks

# Qdrant
qdrant_host: str = "localhost"
qdrant_port: int = 6333
qdrant_collection_name: str = "research_papers"
```

## ğŸ”¬ LlamaIndex Components Used

| Component | Import | Purpose |
|-----------|--------|---------|
| `SimpleDirectoryReader` | `llama_index.core` | Load PDFs |
| `SentenceSplitter` | `llama_index.core.node_parser` | Smart chunking |
| `HuggingFaceEmbedding` | `llama_index.embeddings.huggingface` | Generate embeddings |
| `Document` | `llama_index.core.schema` | Document representation |

## ğŸ“Š Embedding Model

Using **BAAI/bge-base-en-v1.5**:

- **Dimensions**: 768
- **Type**: Dense embeddings
- **Language**: English
- **Quality**: State-of-the-art on MTEB benchmark
- **Cost**: FREE (runs locally)

## ğŸ—ºï¸ Roadmap

- [x] Week 1: PDF â†’ Chunks â†’ Embeddings â†’ Qdrant
- [ ] Week 2: RAG Query Engine with LlamaIndex
- [ ] Week 3: LLM Integration (Response Generation)
- [ ] Week 4: Production Deployment

## ğŸ“ License

MIT License

---

Built with â¤ï¸ using LlamaIndex, HuggingFace, and Qdrant
